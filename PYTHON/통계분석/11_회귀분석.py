#1.단순회귀모형
# 2.다중회귀모형
# 3.모형의 선택
# 4.모형의 타당성
# 
# 회귀분석 : 예측할때 쓰임
# 회귀분석이란 인과관계가 의심되는 복수의 변수를 사용하여
# 어느 변수로부터 다른 변수의 값을 예측하는 기법
# 
# 독립변수: 설명변수 - 원인이되는 변수 (피쳐
# 종속변순: 반응변수 - 결과가 되는 변수 (타겟

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import statsmodels.formula.api as smf

#1. 단순회귀모형 : 독립변수와 종속변수가 1개씩인 가장 단순한 모델
#식으로 표현 y=b0+b1*x

#종속 변수 : 기말고사 점수
#독립 변수 : 쪽지 시험의 평균 점수
df= pd.read_csv('./data/ch12_scores_reg.csv')
df.info()
df.head()

x= np.array(df['quiz'])
y= np.array(df['final_test'])
p=1 #독립 변수의 수
#선형 회귀 식
poly_fit = np.polyfit(x,y,p)
poly_1d = np.poly1d(poly_fit)
xs = np.linspace(x.min(),x.max())
ys = poly_1d(xs)

fig = plt.figure(figsize=(10,6))
ax = fig.add_subplot(111)
ax.set_xlabel('quiz')
ax.set_ylabel('final test')
ax.plot(xs, ys, color='gray', label=f'{poly_fit[1]:.2f}+{poly_fit[0]:.2f}x')
ax.scatter(x,y)
ax.legend()
plt.show()

#(1) 오차에 관한 회귀분석의 가설
#데이터는 직선과 완전히 일치하지 않는다
#종속변수는 확룔변수는 아니다.
#예측할수 없는 부분을 오차항 이라고 하고 ei는 서로 독립이고 N(0, sigma^2)를 따른다.
#Yi = b0 + b1*x + e1(i=1,2,3,.....n)


#ols 모델로 회귀분석
formula = 'final_test ~quiz'
result = smf.ols(formula, df).fit()
result.summary()

#결과
#                             OLS Regression Results
# ==============================================================================
# Dep. Variable:             final_test   R-squared:                       0.676
# Model:                            OLS   Adj. R-squared:                  0.658
# Method:                 Least Squares   F-statistic:                     37.61
# Date:                Thu, 10 Jun 2021   Prob (F-statistic):           8.59e-06
# Time:                        12:37:50   Log-Likelihood:                -76.325
# No. Observations:                  20   AIC:                             156.7
# Df Residuals:                      18   BIC:                             158.6
# Df Model:                           1
# Covariance Type:            nonrobust
# ==============================================================================
#                  coef    std err          t      P>|t|      [0.025      0.975]
# ------------------------------------------------------------------------------
# Intercept     23.6995      4.714      5.028      0.000      13.796      33.603
# quiz           6.5537      1.069      6.133      0.000       4.309       8.799
# ==============================================================================
# Omnibus:                        2.139   Durbin-Watson:                   1.478
# Prob(Omnibus):                  0.343   Jarque-Bera (JB):                1.773
# Skew:                           0.670   Prob(JB):                        0.412
# Kurtosis:                       2.422   Cond. No.                         8.32
# ==============================================================================

# Notes:
# [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
# """
# >>>

#(3) 회귀계수
#intercept : 절편 b0
#quiz : 기울기 b1
#coef : 회귀계수의 추정값
#std err : 추정값의 표준오차(표본평균의 표준편차 (불편분산/개수))
# t : 회귀계수에 대한 t검정 통계량
#p>|t| : t검정통계량에 대한 p값
#[0.025 0.975]: 회귀 계수의 95%신뢰구간



#--------------------------------------------------------------------------------------------------------------
#2. 다중회귀 모형 - 독립변수가 2개 이상인 모형
# y=b0+b1*x1 + b2*x2 + .... + bn*xn
formula = 'final_test ~ quiz + sleep_time'
result = smf. ols (formula, df).fit()
result.summary()

df.head()
#명의 척도 인 탈것
#통학방법 - 질적변수의 명의 척도
#가변수 - 질적변수의 카테고리수에서 하나를 줄인 수만큼 필요
#X도보, X버스
#X도보=1, X버스=0 : 도보
#X도보=0, X버스=1 : 버스
#X도보=0, X버스=0 : 자전거

formula = 'final_test ~ quiz + sleep_time + school_method'
result = smf.ols(formula, df).fit()
result.summary()

#결과
#                        OLS Regression Results
# ==============================================================================
# Dep. Variable:             final_test   R-squared:                       0.782
# Model:                            OLS   Adj. R-squared:                  0.724
# Method:                 Least Squares   F-statistic:                     13.46
# Date:                Thu, 10 Jun 2021   Prob (F-statistic):           7.47e-05
# Time:                        13:41:54   Log-Likelihood:                -72.368
# No. Observations:                  20   AIC:                             154.7
# Df Residuals:                      15   BIC:                             159.7
# Df Model:                           4
# Covariance Type:            nonrobust
# =========================================================================================
#                             coef    std err          t      P>|t|      [0.025      0.975]
# -----------------------------------------------------------------------------------------
# Intercept                 1.3330     12.434      0.107      0.916     -25.169      27.835
# school_method[T.bus]     -1.8118      6.324     -0.286      0.778     -15.292      11.668
# school_method[T.walk]    -7.6555      6.420     -1.192      0.252     -21.339       6.028
# quiz                      6.0029      1.033      5.809      0.000       3.800       8.206
# sleep_time                4.5238      1.809      2.501      0.024       0.668       8.380
# ==============================================================================
# Omnibus:                        1.764   Durbin-Watson:                   1.418
# Prob(Omnibus):                  0.414   Jarque-Bera (JB):                0.989
# Skew:                           0.545   Prob(JB):                        0.610
# Kurtosis:                       2.985   Cond. No.                         41.8
# ==============================================================================

# Notes:

##기말고사 점수는 = 6.0*퀴즈점수 + 4.5*잠잔시간 -1.8통학버스 -7.6*통학걷기+1.33

##################################################################################################################################################################################
#3.모형의 선택
#회귀분석으로 만든 모형 (1)적합이 좋은것, (2)예측이 좋은것
#예측이 좋은 모델이 더 좋은것
#(1) 적합이 좋다는 것은 모형이 주변에 있는 데이터에 잘 들어 맞는다.
#결정계수값이 1에 가깝다
#(2)예측이 좋다는 것은 미지의 데이터를 어느정도 예측할수 있다.
#예측지표 AIC, 값이 작을 수록 좋다.

#(1) 적합이 좋은 모형은 독립변수(설명변수)를 증가시키면 간단하게 달성
#=> 예측 정확도는 떨어짐(과적합)
#독립변수를 증가시킬수록 모형의 복잡도는 올라가고 일반적인 예측성을 잃어 버림
#결론 - 모형선택에는 예측 정확도가 좋은 것을 선택

#(1) 결정계수 (R - squared) 모형의 적합도를 나타내는 지표
#0~1사이의 값을 취하고 1에 가까울수록 데이터에 잘 들어 맞음

x = np.array(df['quiz'])
y = np.array(df['final_test'])
formula = 'final_test ~ quiz'
result = smf.ols(formula, df).fit()
result.summary()

#모형의 예측값
y_hat = np.array(result.fittedvalues)
y_hat

#잔차 = 실제값 - 예측값
eps_hat = np.array(result.resid)
eps_hat

#잔차 제곱의 합
np.sum(eps_hat**2)

#총변동 - 관측값 yi가 어느정도 분산되어있는 지표
#sum((yi-y_mean)^2)

#회귀변동 - 예측값 y_hat_i가 관측값 평균값 y_mean에 대해서
#어느정도 분산되어있느지 나타내는 지표
#예측값이 관측값에 가까워 질수록 총변동과 값이 비슷해짐
#sum((y_hat_i-y_mean)^2)

#잔차변동 - 잔차의 산포도를 나타내는 지표, 잔차 제곱의 합과 동일
#예측값이 관측값에 가까울수록 0에 수렴


#총변동 = 회귀변동 + 잔차변동

#r^2 = 회귀변동 /총변동 = 1-(잔차변동 / 총변동)

total_var = np.sum((y-np.mean(y))**2)#총변동
exp_var = np.sum((y_hat-np.mean(y))**2)#회귀변동
unexp_var = np.sum(eps_hat**2)#잔차변동

total_var,exp_var,unexp_var

#별 의미없는 변수가 들어가도 독립변수가 늘어나면 결정계수 증가
#(2) 조정결정계수
#adj R-squared, R_bar^2
#독립변수를추가했을때 종속변수에 어느정도 이상의 설명력이 없는경우
#결정계수의 값이 증가하지 않도록 조정하는 결정계수
#R_bar^2 = 1-(잔차변동/(n-p-1)/(총변동/(n-1)))


#(3) F검정(모형적합도를 나타내는 지표)
#귀무가설 : b1=b2=b3=....=bn =0
#대립가설 : 적어도 하나의 b값은 0이 아니다.
#F검정은 t검정과 다르게 개개의 회귀계수에 대해서가 아니라 모형전체에 대해서 수행됨
#F=(회귀변동 / p )/(잔차변동 /(n-p-1))
p=1
n=len(df)
f = (exp_var/p) / (unexp_var/(n-p-1))
f

#(4)최대로그우도와 AIC
#AIC - 모형의 예측성능에 관한 지표
#우도는 어떤 관측값을 얻을 확률
#최대 우도는 우도함수(확률함수)가 최대가 되는 값
#최대로그우도 : 최대우도 로그를 취한값
#최대로그우도 가 클수록 적합도가 높다고 생각할수 있다.


#AIC = -2*최대로그우도 + 2*회귀계수의 수
#AIC 는 작을수록 예측도가 좋다고 생각할 수 있다.
#베이지안 정보 기준
#AIC와 유사한 지표 - 값이 작을수록 좋음
#BIC = -2*최대로그우도 + logn*회귀계수의 수



######################################################################################################################################################################
#4.모형의 타당성
#회귀분석에 관해서 세운 오차항 ei는 서로 독립이고 N(0,sigma^2)
#를 따른다 라는 가정을 만족하고 있는지 여부 체크

#(1) 정규성 검정
#귀무가설 : 잔차항은 정규분포를 따른다
#대립가설 : 잔차항은 정규분포를 따르지 않는다.
#Omnibus 검정과 jarque-Bera 검정이 사용
#p값이 0.05보다 크면 문제 될것이 없음

#왜도(skew) : 분포의 좌우대칭을측정하는 지표
#왜도가 0이면 정규분포처럼 좌우대칭
#왜도가 0보다 크면 카이제곱분포처럼 왼쪽으로 치우침
#왜도가 0보다 작으면 오른쪽으로 치우친 분포

#첨도(Kurtosis) : 분포의 뽀족한 정도
#정규분포의 첨도 3
#3보다 크면 정규분포보다 뽀족함
#3보다 작으면 정규분포보다 둥글다.
#위식에서 -3을해서 첨도를 정의하는 경우도 있음

#(2) 더빈 - 왓슨비
# 다른 오차항이 서로 무상관인지 체크하는 지표
#시계열 데이터의 경우에 중요한 지표
#0~4의 값을 갖고 0에 가까우면 양의 상관, 4에 가까우면 음의 상관
#2에 가까우면 무상관

#(3) 다중공선성
#Cond. No. (조건수)
#회귀분석에 사용된 모형의 일부 독립변수가 다른 독립변수와 
#상관정도가 높아 데이터 분석시 부정적인 영향을 미치는 현상

#조건수 값이 크면 독립변수 사이에 강한 상관이 생겼다는 것을 의미
#다중공선성이 크면 회귀 계수의 분산이 커져 모형의 예측결과가 나빠짐

#해결 방법 : 상관있는 독립변수중 하나를 뺀다.




